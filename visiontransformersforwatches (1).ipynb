{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8164852,"sourceType":"datasetVersion","datasetId":4831241},{"sourceId":8165596,"sourceType":"datasetVersion","datasetId":4831781},{"sourceId":8175983,"sourceType":"datasetVersion","datasetId":4839676},{"sourceId":8184095,"sourceType":"datasetVersion","datasetId":4845841}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys \nsys.path.append(\"/kaggle/input/einops/einops-0.7.0\") \nimport einops\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nfrom PIL import Image\nimport os\nfrom torchvision.datasets.folder import default_loader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T10:42:02.283627Z","iopub.execute_input":"2024-04-21T10:42:02.284450Z","iopub.status.idle":"2024-04-21T10:42:09.093299Z","shell.execute_reply.started":"2024-04-21T10:42:02.284406Z","shell.execute_reply":"2024-04-21T10:42:09.092422Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def pair(t):\n    if isinstance(t, tuple):\n        return t\n    return t, t\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout = 0.):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            Mish(),  # Using Mish activation function as defined\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n        super().__init__()\n        inner_dim = dim_head * heads\n        project_out = not (heads == 1 and dim_head == dim)\n\n        self.heads = heads\n        self.scale = dim_head ** -0.5\n\n        self.attend = nn.Softmax(dim = -1)\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\n        self.to_out = nn.Sequential(\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        ) if project_out else nn.Identity()\n\n    def forward(self, x):\n        qkv = self.to_qkv(x).chunk(3, dim = -1)\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\n        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\n        attn = self.attend(dots)\n\n        out = torch.matmul(attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        return self.to_out(out)\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n            ]))\n\n    def forward(self, x):\n        for attn, ff in self.layers:\n            x = attn(x) + x\n            x = ff(x) + x\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n        super().__init__()\n        image_height, image_width = pair(image_size)\n        patch_height, patch_width = pair(patch_size)\n\n        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n\n        num_patches = (image_height // patch_height) * (image_width // patch_width)\n        patch_dim = channels * patch_height * patch_width\n        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n\n        self.to_patch_embedding = nn.Sequential(\n            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n            nn.Linear(patch_dim, dim),\n        )\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.dropout = nn.Dropout(emb_dropout)\n\n        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n\n        self.pool = pool\n        self.to_latent = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(dim),\n            nn.Linear(dim, num_classes)\n        )\n\n    def forward(self, img):\n        x = self.to_patch_embedding(img)\n        b, n, _ = x.shape\n\n        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding[:, :(n + 1)]\n        x = self.dropout(x)\n\n        x = self.transformer(x)\n\n        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n\n        x = self.to_latent(x)\n        return self.mlp_head(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:43:45.726606Z","iopub.execute_input":"2024-04-21T10:43:45.727180Z","iopub.status.idle":"2024-04-21T10:43:45.752561Z","shell.execute_reply.started":"2024-04-21T10:43:45.727147Z","shell.execute_reply":"2024-04-21T10:43:45.751532Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    \n])\n\ndataset = datasets.ImageFolder(root='/kaggle/input/smart-watc/training', transform=transform)\n\nbatch_size = 32\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Initialize the ViT model\nmodel = ViT(\n    image_size=224,\n    patch_size=16,\n    num_classes=2,  # Assuming 2 classes: defective and normal\n    dim=512,\n    depth=6,\n    heads=8,\n    mlp_dim=1024,\n    pool='cls',\n    channels=3,\n    dim_head=64,\n    dropout=0.1,\n    emb_dropout=0.1\n)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nnum_epochs = 15\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    for images, labels in data_loader:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    # Print epoch loss\n    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(data_loader):.4f}')\ntorch.save(model.state_dict(), 'vision_transformer.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T10:44:16.544340Z","iopub.execute_input":"2024-04-21T10:44:16.544736Z","iopub.status.idle":"2024-04-21T11:37:30.591840Z","shell.execute_reply.started":"2024-04-21T10:44:16.544706Z","shell.execute_reply":"2024-04-21T11:37:30.590744Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Epoch [1/15], Loss: 0.7554\nEpoch [2/15], Loss: 0.6713\nEpoch [3/15], Loss: 0.6679\nEpoch [4/15], Loss: 0.6766\nEpoch [5/15], Loss: 0.6518\nEpoch [6/15], Loss: 0.6460\nEpoch [7/15], Loss: 0.6197\nEpoch [8/15], Loss: 0.6312\nEpoch [9/15], Loss: 0.6073\nEpoch [10/15], Loss: 0.5860\nEpoch [11/15], Loss: 0.6072\nEpoch [12/15], Loss: 0.5864\nEpoch [13/15], Loss: 0.5551\nEpoch [14/15], Loss: 0.5484\nEpoch [15/15], Loss: 0.5356\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_normal = \"/kaggle/input/test-acc/Test/normal\"\ntest_defect = \"/kaggle/input/test-acc/Test/defective\"\n\nmodel = ViT(\n    image_size=224,\n    patch_size=16,\n    num_classes=2, \n    dim=512,\n    depth=6,\n    heads=8,\n    mlp_dim=1024,\n    pool='cls',\n    channels=3,\n    dim_head=64,\n    dropout=0.1,\n    emb_dropout=0.1\n)\nmodel.load_state_dict(torch.load('vision_transformer.pth'))\nmodel.eval()\n\ndef test_model(model, test_dir):\n    correct = 0\n    total = 0\n    for image_file in os.listdir(test_dir):\n        image_path = os.path.join(test_dir, image_file)\n        image = Image.open(image_path).convert(\"RGB\")\n        input_image = transform(image).unsqueeze(0)  # Add batch dimension\n        \n        with torch.no_grad():\n            output = model(input_image)\n            probabilities = torch.softmax(output, dim=1)\n            predicted_class = torch.argmax(probabilities, dim=1).item()\n            \n            ground_truth_label = \"defective\" if \"defective\" in image_file else \"normal\"\n            l\n            if (predicted_class == 0 and ground_truth_label == \"defective\") or \\\n               (predicted_class == 1 and ground_truth_label == \"normal\"):\n                correct += 1\n            \n            total += 1\n    \n    accuracy = correct / total * 100\n    return accuracy\n\naccnormal = test_model(model, test_normal)\naccdefect = test_model(model, test_defect)\naccuracy = (accnormal+accdefect)/2\nprint(f\"Accuracy: {accuracy:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-04-21T11:57:12.942811Z","iopub.execute_input":"2024-04-21T11:57:12.943237Z","iopub.status.idle":"2024-04-21T11:57:31.911232Z","shell.execute_reply.started":"2024-04-21T11:57:12.943204Z","shell.execute_reply":"2024-04-21T11:57:31.910014Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 94.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}